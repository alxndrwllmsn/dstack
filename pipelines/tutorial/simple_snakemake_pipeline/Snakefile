"""Snakemake pipelines are defined in a file called Snakefile.
By default snakemake looks for the snakefile in the local directory when
it is called. However, the snakefile an be named differently and 
can be in a different location. However, for simplicity each pipeline
defined should be stored within its own folder with a file defining the
pipeline named Sankefile.

Sakemake uses a Python-like syntax and actual python code
can be called inside the Snakefile as well as python object
can be used.

This pipeline defines the skeleton of the most simple
grid stacking possible: creating and averaging two grids, and
finally image the result. Since it is a simple example, text
files are used to represent the images parset files and grids.

For further information see the README file.
"""
#=== Imports ===
# We can import python stuff
import numpy as np
import os

#=== Define general parameters ===
# General parameters for the pipeline can be defined here
N_chan = 3 #Number of channels processed.
# Currently all channels processed simultaneously
# The data is not imaged in separate spectral windows

# We does not use this parameter in this example, but
# it is here for the reader as an exercise to include
# it to the pipeline by writing it to the final stacked
# image!

#=== Define the envinroment varables ===
WORKING_DIR = os.getcwd()

# The pipeline runs by default in the current working
# directory, and so input and output can be defined
# relative to the current working directory where the
# Snakefile lives

# It is also possible to change the working directory
# relative to the Snakefile directory, but that makes
# the pipeline more messy... so for now we stick to the
# default relative path settings.

#=== Input data mapping ===
# To work with irregularly named input data a mapping to
# regular variables is needed
# I use a dictionary to do this

INPUT_DATA_MAPPING = {'0' : 'MS_1',
                    '1' : 'second_MS'}

#=== RULES ===
# Steps of a snakemake pipeline are called rules and
# each pipeline is build frominput_data top-to-bottom fashion
# similarly to gnu make.
# Ergo the pipeline looks for the output of the last
# step to be executed by the pipeline first. Then it
# searches for the rules providing the input data to
# that rule or if the input files are exists. If another
# rule creates the input file the snakemake looks at
# that rule until if founds the input existing on the
# file system.
# Each rule has to have an input file and an output file
# associated.
# The steps can execute command-line applications or small
# python snippets.

# Here I define the rules in order how snakemake are
# looking for them and building up the pipeline

#=== The last pipeline step 
# This rule only looking for the final output
rule all:
    input:
        'output_data/stacked_image.txt'

# Note that the absolute path defined by me, otherwise the
# path used would be relative to the folder where the Snakefile lives

#=== Deep imaging
# The stacked image is generated by the deep imaging step
# It require the stacked grid as an input
rule deep_imaging:
    input:
        'output_data/stacked_grid.txt'
    output:
        'output_data/stacked_image.txt'
    shell:
        # Define that the rule calls a shell command that needs to be a
        # single python string like this
        'touch {0:s}/output_data/stacked_image.txt'.format(WORKING_DIR) \
        + ' && ' + 'echo This is the final stacked image >> \
        {0:s}/output_data/stacked_image.txt'.format(WORKING_DIR)

# NOTE that a logical step of creating a deep parset is left out here!
# this is to keep the example simple (can think of the gridding step as
# creating the parset for the deep imaging with all the input grids)

#=== Grid stacking
# The grid stacking has two input grids, for which we
# gonna provide as a list of the two input grids
# this is a gather step and for this we need to know
# the grid indices used, that we are gonna derive from the
# input mapping
rule grid_stacking:
    input:        
        input_grids = ['output_data/grid_{0:s}.txt'.format(grid_index) for grid_index in INPUT_DATA_MAPPING.keys()] #The input data can be named and used as a variable
    output:
        output_data = 'output_data/stacked_grid.txt' #Similarly the output can be named as well
    shell:
        'touch {0:s}/output_data/stacked_image.txt'.format(WORKING_DIR) \
        + ' && ' + 'echo This is the stacked grid >> \
        {0:s}/output_data/stacked_grid.txt'.format(WORKING_DIR) \
        + '&& echo The grids used for stacking: {input.input_grids} >> ' \
        + '{0:s}/output_data/stacked_grid.txt'.format(WORKING_DIR) #Input values and wildcards and general variables cannot be mixed within one python string, so the final string needs to be sliced to mix these variables!


#=== Gridding the input data
# In this rule we don't want to define each input and output
# with a hard coded index. That is only needed for the gather
# step, since the input of the gather step has to be an explicit
# list. Furthermore, we want to run the gridding step independently
# on each input day! Therefore, we are gonna use wildcards.
# Wildcards can be defined with the {wildcard} syntax.
# There are some rules how to do this properly, but for that
# see the snakemake documentation. Here we use 'general'
# wildcards as grid indices that will be linked to the mapping
# of the input data at some point.
rule gridding:
    input:
        'output_data/gridding_parset_{data_index}.txt' #The input file is defined by the wildcard value
    params:
        data_no = '{data_index}', #Wildcards can be used in the parameters as well
        meaningless = 100 #This can be any variable, but need to convert to a string when running the shell command
    output:
        'output_data/grid_{data_index}.txt' #The output file name is defined by the wildcard value
    shell:
        'touch {0:s}/output_data/'.format(WORKING_DIR) \
        + 'grid_{wildcards.data_index}.txt && ' \
        + 'echo Data no. {params.data_no} gridded with \
        meaningless parameter of {params.meaningless}! >>' \
        + '{0:s}/output_data/'.format(WORKING_DIR) \
        + 'grid_{wildcards.data_index}.txt' #Note that in the shell script wildcard values are called differently from when they called in the parameter definition!
 
#=== Create parset for gridding
# This rule creates a parset file for each input data
# defined by the original mapping. Ergo, the wildcard
# values are defined here using a lambda function syntax.
# This is how parallel input can be defined for
# a snakemake pipeline
rule create_parset:
    input:
        MS = lambda wildcards: '{0:s}/input_data/{1:s}.txt'.format(WORKING_DIR,INPUT_DATA_MAPPING[wildcards.data_index]) #I define the wildcards here with a lambda function
    output:
        'output_data/gridding_parset_{data_index}.txt'
    shell:
        'touch {0:s}/output_data/'.format(WORKING_DIR) \
        + 'gridding_parset_{wildcards.data_index}.txt && ' \
        + 'echo Parset file used for gridding >> {0:s}/output_data/'.format(WORKING_DIR) \
        + 'gridding_parset_{wildcards.data_index}.txt'


